{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import nibabel \n",
    "\n",
    "from nilearn.glm.first_level import run_glm\n",
    "from nilearn.glm.contrasts import compute_contrast\n",
    "from nilearn.glm.thresholding import fdr_threshold\n",
    "from nilearn import image as nimg\n",
    "from nilearn import plotting \n",
    "from nilearn.plotting import view_surf\n",
    "from nilearn.plotting import plot_surf_stat_map\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.plotting import plot_glass_brain\n",
    "\n",
    "%matplotlib inline\n",
    "import hcp_utils as hcp\n",
    "from hcp_utils import left_cortex_data, right_cortex_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 3 group means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_average_contrast(contrast,ses,group,task,covariates,subgroup,rm_CUD,desired_outliers):\n",
    "    print('Now processing: '+ ' '.join([contrast,ses,group,task]))\n",
    "    \n",
    "    #check if mriqc summary exists and find runs that need to be excluded based on desired outlier cutoffs\n",
    "    if desired_outliers:\n",
    "        if len(glob.glob(f'../../../derivatives/mriqc_summaries/group-{group}_ses-{ses}_task-{task}_rec-unco.tsv'))==0:\n",
    "            print('No mriqc summary found. Please create this before requesting to remove outliers.')\n",
    "            return (None, None)\n",
    "\n",
    "    excluded_subs_runs_dict = get_excluded_runs(desired_outliers,contrast,ses,group,task)\n",
    "    \n",
    "    effect_size_maps, all_subs = get_effect_size_maps(contrast,ses,group,task,excluded_subs_runs_dict)\n",
    "    \n",
    "    #remove participants with CUD at baseline (only applies to MM)\n",
    "    if rm_CUD:\n",
    "        effect_size_maps, all_subs = rm_CUD_baseline(effect_size_maps, all_subs)\n",
    "        \n",
    "    \n",
    "    #ensure we have data\n",
    "    if not effect_size_maps:\n",
    "        print('No effect size maps, so cannot generate group level output for: '+' '.join([contrast,ses,group,task]))\n",
    "        return (None, None)\n",
    "    \n",
    "    #note that subject formatting is changed slightly to match that of the non_imaging_data csv file\n",
    "    #need these to make design matrix\n",
    "    subs_csv = ['_'.join([s for s in re.split(r'(MM|HC)', sub) if s]) for sub in all_subs]\n",
    "    \n",
    "    \n",
    "    if subgroup:\n",
    "        effect_size_maps, subs_csv, all_subs = select_subgroup(subgroup, effect_size_maps, subs_csv, all_subs)\n",
    "\n",
    "    \n",
    "    #needed for printing total later\n",
    "    part_count = len(subs_csv)\n",
    "    \n",
    "    #create design matrix including all covariates and a way to encode group average (either as col of 1s or with Male/Female)\n",
    "    group_design_matrix = create_group_design_matrix(subs_csv,group,ses,covariates)\n",
    "    \n",
    "\n",
    "    effect_size_signals = [nimg.load_img(dscalar).get_fdata(dtype='f4') for dscalar in effect_size_maps]\n",
    "        \n",
    "    effect_size_signals_combined = np.vstack(effect_size_signals)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #set up and run glm\n",
    "    #note that we are not smoothing with nilearn since the data was smoothed using the HCP pipeline\n",
    "    labels, estimates = run_glm(effect_size_signals_combined, group_design_matrix.values, noise_model='ols', \n",
    "                                n_jobs=-2, verbose=0)\n",
    "\n",
    "    \n",
    "    #define contrast matrix \n",
    "    contrast_matrix = np.eye(group_design_matrix.shape[1])\n",
    "    grp_contrasts = dict([(column, contrast_matrix[i])\n",
    "                      for i, column in enumerate(group_design_matrix.columns)])\n",
    "    \n",
    "    print(grp_contrasts)\n",
    "\n",
    "    \n",
    "    contrast_outputs = {}\n",
    "    \n",
    "    for key in grp_contrasts.keys():\n",
    "        #compute the group contrasts\n",
    "        contrast_output = compute_contrast(labels, estimates, grp_contrasts[key], contrast_type='t')\n",
    "        \n",
    "        contrast_outputs[key] = contrast_output\n",
    "\n",
    "    \n",
    "    #for printing the covariates\n",
    "    if covariates == '':\n",
    "        covariates = 'nothing'\n",
    "        \n",
    "    print('Processing done for: '+' '.join([contrast,ses,group,task]))\n",
    "    print(f'Controlled for {covariates}')\n",
    "    \n",
    "    return (contrast_outputs, part_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae962fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excluded_runs(desired_outliers,contrast,ses,group,task):\n",
    "    \n",
    "    #empty list and dict to save runs to be excluded\n",
    "    excluded_runs=[]\n",
    "    excluded_runs_dict={}\n",
    "    \n",
    "    if len(desired_outliers.keys())==0:\n",
    "        return excluded_runs_dict\n",
    "    \n",
    "    #read in cutoffs based on Q1-1.5*IQR and Q3+1.5*IQR based on all tasks and groups (not just current) for this session\n",
    "    iqr_cutoffs_df = pd.read_csv(f'../../../derivatives/mriqc_summaries/iqr_cutoffs_ses-{ses}_rec-unco.tsv', low_memory=False, sep='\\t')\n",
    "    \n",
    "    #read in tsv file with mriqc summary for current task, ses, group combi\n",
    "    mriqc_summary = pd.read_csv(f'../../../derivatives/mriqc_summaries/group-{group}_ses-{ses}_task-{task}_rec-unco.tsv', low_memory=False, sep='\\t')\n",
    "    \n",
    "    \n",
    "    #possible outlier types that use IQR-based cutoff: tsnr, snr, gsr_x, gsr_y\n",
    "    #exclude runs based on these if in desired_outliers list\n",
    "    for outlier_type in ['tsnr', 'snr', 'gsr_x', 'gsr_y']:\n",
    "        if outlier_type in desired_outliers.keys():\n",
    "            #get upper and lower cutoffs from iqr summaries df\n",
    "            lower_bound = iqr_cutoffs_df[outlier_type].iloc[0]\n",
    "            upper_bound = iqr_cutoffs_df[outlier_type].iloc[1]\n",
    "            \n",
    "            #find subs_runs combinations outside of cutoffs \n",
    "            above_subs_runs = mriqc_summary['subs_runs'][mriqc_summary[outlier_type]>=upper_bound].tolist()\n",
    "            below_subs_runs = mriqc_summary['subs_runs'][mriqc_summary[outlier_type]<=lower_bound].tolist()\n",
    "            subs_runs = above_subs_runs+below_subs_runs\n",
    "            excluded_runs+=subs_runs\n",
    "            \n",
    "    if 'fd_mean' in desired_outliers.keys():\n",
    "        above_fd_mean_runs = mriqc_summary['subs_runs'][mriqc_summary['fd_mean']>desired_outliers['fd_mean']].tolist()\n",
    "        if 'fd_perc' in desired_outliers:\n",
    "            below_fd_perc_runs = set(mriqc_summary['subs_runs'][mriqc_summary['fd_perc']<desired_outliers['fd_perc']].tolist())\n",
    "            above_fd_mean_runs = set(above_fd_mean_runs)\n",
    "            above_fd_mean_runs = list(above_fd_mean_runs.difference(below_fd_perc_runs))\n",
    "        excluded_runs+=above_fd_mean_runs\n",
    "        \n",
    "    if 'motion_outlier_cutoff' in desired_outliers.keys():\n",
    "        perc_cutoff =  desired_outliers['motion_outlier_cutoff']\n",
    "        \n",
    "        if task == 'nback':\n",
    "            n_scans = 278\n",
    "        elif task == 'mid':\n",
    "            n_scans = 215\n",
    "        elif task == 'sst':\n",
    "            n_scans = 257\n",
    "        \n",
    "        motion_outlier_df = pd.read_csv(f'../../../derivatives/motion_outlier_counts/{task}_motion_outlier_count.csv', low_memory=False)\n",
    "        motion_outlier_above_cutoff_runs = motion_outlier_df['subs_runs'][motion_outlier_df['motion_outlier_count']>(n_scans*perc_cutoff)].tolist()\n",
    "    \n",
    "        excluded_runs+=motion_outlier_above_cutoff_runs\n",
    "        \n",
    "        \n",
    "    #remove duplicates\n",
    "    excluded_runs=list(set(excluded_runs))\n",
    "    \n",
    "    #turn into dict\n",
    "    for sub_run in excluded_runs:\n",
    "        sub = sub_run.split('_')[0]\n",
    "        run = sub_run.split('_')[1]\n",
    "        if sub in excluded_runs_dict.keys():\n",
    "            excluded_runs_dict[sub].append(run)\n",
    "        else:\n",
    "            excluded_runs_dict[sub]=[run]\n",
    "    \n",
    "    return excluded_runs_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ee165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_CUD_baseline(effect_size_maps, all_subs):\n",
    "    \n",
    "    #subs to be excluded (only MM) because they had cannabis use disorder at baseline (exclusion criterium)\n",
    "    excluded_subs = ['MM014','MM188','MM197','MM217','MM228','MM239','MM241']\n",
    "    \n",
    "    #get only effect size maps that aren't those of any of the excluded subjects\n",
    "    final_effect_size_maps = [path for path in effect_size_maps if path.split('/sub-')[1].split('/')[0] not in excluded_subs]\n",
    "\n",
    "    #get only subjects that aren't those of any of the excluded subjects\n",
    "    final_all_subs = [sub for sub in all_subs if sub not in excluded_subs]\n",
    "\n",
    "    return final_effect_size_maps, final_all_subs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subgroup(subgroup, effect_size_maps, subs_csv, subs):\n",
    "    non_img_data = pd.read_csv(f\"../../../sourcedata/non_imaging_data/MMJ-Processed_data-2022_05_27-13_58-6858bbe.csv\",low_memory=False)\n",
    "    final_subs = []\n",
    "    final_subs_csv = []\n",
    "    final_effect_size_maps = []\n",
    "    if subgroup == 'dipstick_THC':\n",
    "        dipstick_THC_results_dict = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'One year'].groupby('IDS.CHR.Subject')['URN.LGC.THC_present'].agg(\"first\").to_dict()\n",
    "        for i in range(len(effect_size_maps)):\n",
    "            if subs_csv[i] in dipstick_THC_results_dict.keys():\n",
    "                if dipstick_THC_results_dict[subs_csv[i]]:\n",
    "                    final_subs.append(subs[i])\n",
    "                    final_subs_csv.append(subs_csv[i])\n",
    "                    final_effect_size_maps.append(effect_size_maps[i])\n",
    "    \n",
    "                \n",
    "    return (final_effect_size_maps, final_subs_csv, final_subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size_maps(contrast,ses,group,task,excluded_subs_runs_dict):\n",
    "    \n",
    "    #get individual effect size maps\n",
    "    #get all second-level contrasts that exist\n",
    "    second_level_effect_size_maps = glob.glob(f'../../../derivatives/task_analysis_surface/second_level/sub-{group}*/ses-{ses}/task-{task}/sub-{group}*_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_effect_size_fx.dscalar.nii')\n",
    "    second_level_subs = [path.split('/sub-')[1].split('/')[0] for path in second_level_effect_size_maps if path]\n",
    "    second_level_dict = dict(zip(second_level_subs, second_level_effect_size_maps))\n",
    "\n",
    "    #remove any second level contrasts who have at least one run that needs to be excluded \n",
    "    for sub in excluded_subs_runs_dict.keys():\n",
    "        if sub in second_level_dict.keys():\n",
    "            second_level_dict.pop(sub)\n",
    "    \n",
    "    second_level_subs = list(second_level_dict.keys())\n",
    "    second_level_effect_size_maps = list(second_level_dict.values())\n",
    "    \n",
    "    print(f'second level data subs count after any outlier removal: {len(second_level_subs)}')\n",
    "    \n",
    "    #get first-level contrasts for the remaining subjects\n",
    "    all_first_level_effect_size_maps = glob.glob(f'../../../derivatives/task_analysis_surface/first_level/sub-{group}*/ses-{ses}/task-{task}/sub-{group}*_ses-{ses}_task-{task}_rec-unco_run-*_contrast-{contrast}_effect_size.dscalar.nii')\n",
    "    first_level_effect_size_maps = [path for path in all_first_level_effect_size_maps if path.split('/sub-')[1].split('/')[0] not in second_level_subs]\n",
    "    first_level_subs = [path.split('/sub-')[1].split('/')[0] for path in first_level_effect_size_maps if path]\n",
    "    first_level_runs = [path.split('run-')[1].split('_')[0] for path in first_level_effect_size_maps if path]\n",
    "\n",
    "    #need to work with sub_run keys since otherwise we'd have duplicate keys\n",
    "    first_level_subs_runs = ['_'.join(x) for x in zip(first_level_subs,first_level_runs)]\n",
    "    first_level_dict = dict(zip(first_level_subs_runs, first_level_effect_size_maps))\n",
    "\n",
    "    for sub,run_list in excluded_subs_runs_dict.items():\n",
    "        for run in run_list:\n",
    "            sub_run = f'{sub}_{run}'\n",
    "            if sub_run in first_level_dict.keys():\n",
    "                first_level_dict.pop(sub_run)\n",
    "    \n",
    "    first_level_subs = [sub_run.split('_')[0] for sub_run in first_level_dict.keys()]\n",
    "    first_level_effect_size_maps = list(first_level_dict.values())\n",
    "    \n",
    "    print(f'first level data subs count after any outlier removal: {len(first_level_subs)}')\n",
    "    \n",
    "    #combine the effect_size_maps into one list and the subjects into one list\n",
    "    effect_size_maps = second_level_effect_size_maps + first_level_effect_size_maps\n",
    "    \n",
    "    all_subs = second_level_subs + first_level_subs\n",
    "    \n",
    "    print(f'all subs count after any outlier removal: {len(all_subs)}')\n",
    "    \n",
    "    return (effect_size_maps, all_subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_design_matrix(subs,group,ses,covariates):\n",
    "    \n",
    "    df_subs = pd.DataFrame(subs,columns=['subs'])\n",
    "    non_img_data = pd.read_csv(f\"../../../sourcedata/non_imaging_data/MMJ-Processed_data-2022_05_27-13_58-6858bbe.csv\",low_memory=False)\n",
    "\n",
    "    #create design matrix with 1 column of 1s and as many rows as there are subjects\n",
    "    group_design_matrix = pd.DataFrame([1] * len(subs), columns=['group_average'],)\n",
    "    \n",
    "    if 'sex' in covariates:\n",
    "        #add columns for male and female, that will then be combined to create the group average \n",
    "        grouped_sex = non_img_data.groupby(\"IDS.CHR.Subject\")[\"SBJ.CHR.Sex\"].agg(\"first\")\n",
    "        dict_sex = grouped_sex.to_dict()\n",
    "        df_subs['sex'] = df_subs['subs'].map(dict_sex)\n",
    "        dummy_df = pd.get_dummies(df_subs['sex'])\n",
    "        group_design_matrix['sex'] = dummy_df['Male']\n",
    "        \n",
    "\n",
    "    #numericals can be added directly\n",
    "    #mean center numerical values like age and CUDIT!\n",
    "\n",
    "    if 'age' in covariates:\n",
    "        #numerical can be added directly\n",
    "        #mean center numerical value\n",
    "        grouped_age = non_img_data.groupby(\"IDS.CHR.Subject\")[\"SBJ.INT.Age\"].agg(\"first\")\n",
    "        dict_age = grouped_age.to_dict()\n",
    "        group_design_matrix['age'] = df_subs['subs'].map(dict_age)\n",
    "        group_design_matrix['age'] = group_design_matrix['age'] - group_design_matrix['age'].mean()\n",
    "    \n",
    "    if 'total_cudit' in covariates:\n",
    "        if group == 'HC': \n",
    "            grouped_HC_baseline_cudit = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Screening'].groupby('IDS.CHR.Subject')['INV.INT.CUDIT.Summed_score'].agg(\"first\")\n",
    "            dict_HC_baseline_cudit = grouped_HC_baseline_cudit.to_dict()\n",
    "            group_design_matrix['total_cudit'] = df_subs['subs'].map(dict_HC_baseline_cudit)\n",
    "            group_design_matrix['total_cudit'] = group_design_matrix['total_cudit'] - group_design_matrix['total_cudit'].mean()\n",
    "\n",
    "        else:\n",
    "            if ses == 'baseline':\n",
    "                dict_MM_baseline_cudit = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Baseline'].groupby('IDS.CHR.Subject')['INV.INT.CUDIT.Summed_score'].agg(\"first\").to_dict()\n",
    "                group_design_matrix['total_cudit'] = df_subs['subs'].map(dict_MM_baseline_cudit)\n",
    "                group_design_matrix['total_cudit'] = group_design_matrix['total_cudit'] - group_design_matrix['total_cudit'].mean()\n",
    "                #needed because MM_141 is nan, so replacing with 0, which is the mean\n",
    "                group_design_matrix['total_cudit'].fillna(0, inplace=True)\n",
    "            else:\n",
    "                dict_MM_1year_cudit = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'One year'].groupby('IDS.CHR.Subject')['INV.INT.CUDIT.Summed_score'].agg(\"first\").to_dict()\n",
    "                group_design_matrix['total_cudit'] = df_subs['subs'].map(dict_MM_1year_cudit)\n",
    "                group_design_matrix['total_cudit'] = group_design_matrix['total_cudit'] - group_design_matrix['total_cudit'].mean()\n",
    "    \n",
    "    #encode the correspondence \n",
    "    freq_dict = {'Once or more per day':7,\n",
    "        '5-6 days a week':6,\n",
    "        '3-4 days a week':5,\n",
    "        '1-2 days a week':4,\n",
    "        'Less than once a week':3,\n",
    "        'Less than once every two weeks':2,\n",
    "        'Less than once a month':1,\n",
    "        None:0}\n",
    "    \n",
    "    if 'THC_freq_month' in covariates:\n",
    "        if group == 'HC':\n",
    "            #results from screening visit (using this for consistency since CUDIT-R was also collected at screening visit)\n",
    "            dict_HC_screening_THC = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Screening'].groupby('IDS.CHR.Subject')['TLF.CHR.THC.Frequency_in_month'].agg(\"last\").to_dict()\n",
    "            dict_HC_screening_THC_num = {k:freq_dict[v] for k,v in dict_HC_screening_THC.items()}\n",
    "            group_design_matrix['THC_freq_month'] = df_subs['subs'].map(dict_HC_screening_THC_num)\n",
    "            group_design_matrix['THC_freq_month'] = group_design_matrix['THC_freq_month'] - group_design_matrix['THC_freq_month'].mean()\n",
    "        else:\n",
    "            if ses == 'baseline':     \n",
    "                #results from MRI visit (using this for consistency since CUDIT-R was also collected at MRI visit)\n",
    "                dict_MM_MRIvisit_THC = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Baseline'].groupby('IDS.CHR.Subject')['TLF.CHR.THC.Frequency_in_month'].agg(\"first\").to_dict()\n",
    "                dict_MM_MRIvisit_THC_num = {k:freq_dict[v] for k,v in dict_MM_MRIvisit_THC.items()}\n",
    "                group_design_matrix['THC_freq_month'] = df_subs['subs'].map(dict_MM_MRIvisit_THC_num)\n",
    "                group_design_matrix['THC_freq_month'] = group_design_matrix['THC_freq_month'] - group_design_matrix['THC_freq_month'].mean()\n",
    "            else:\n",
    "                dict_MM_MRIvisit_THC = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'One year'].groupby('IDS.CHR.Subject')['TLF.CHR.THC.Frequency_in_month'].agg(\"first\").to_dict()\n",
    "                dict_MM_MRIvisit_THC_num = {k:freq_dict[v] for k,v in dict_MM_MRIvisit_THC.items()}\n",
    "                group_design_matrix['THC_freq_month'] = df_subs['subs'].map(dict_MM_MRIvisit_THC_num)\n",
    "                group_design_matrix['THC_freq_month'] = group_design_matrix['THC_freq_month'] - group_design_matrix['THC_freq_month'].mean()\n",
    "\n",
    "                \n",
    "    #toggle if desired\n",
    "    #plot_design_matrix(group_design_matrix)\n",
    "    \n",
    "    return group_design_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_threshold_map(z_map):\n",
    "    #alpha=.05, height_control='fdr'\n",
    "    threshold = fdr_threshold(z_map, alpha=.05)\n",
    "    #two-sided cutoff\n",
    "    z_map[np.abs(z_map) < threshold] = 0\n",
    "    return (z_map, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from neurohackademy cifti tutorial\n",
    "def volume_from_cifti(data, axis):\n",
    "    assert isinstance(axis, nibabel.cifti2.BrainModelAxis)\n",
    "    data = data.T[axis.volume_mask]                          # Assume brainmodels axis is last, move it to front\n",
    "    volmask = axis.volume_mask                               # Which indices on this axis are for voxels?\n",
    "    vox_indices = tuple(axis.voxel[axis.volume_mask].T)      # ([x0, x1, ...], [y0, ...], [z0, ...])\n",
    "    vol_data = np.zeros(axis.volume_shape + data.shape[1:],  # Volume + any extra dimensions\n",
    "                        dtype=data.dtype)\n",
    "    vol_data[vox_indices] = data                             # \"Fancy indexing\"\n",
    "    return nibabel.Nifti1Image(vol_data, axis.affine)             # Add affine for spatial interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coronal_slices(threshold_map, threshold, MNI_template_used, contrast, group, ses, part_count):\n",
    "    \n",
    "    #get any one since they're all the same\n",
    "    brain_model_axis = nibabel.load(glob.glob(f'../../../derivatives/task_analysis_surface/first_level/sub-MM*/ses-1year/task-{task}/sub-MM*_ses-1year_task-{task}_rec-unco_run-*_contrast-*_effect_size.dscalar.nii')[0]).header.get_axis(1)\n",
    "\n",
    "    #make volume from subcortical part of cifti\n",
    "    volume_threshold_map = volume_from_cifti(threshold_map, brain_model_axis)\n",
    "    \n",
    "\n",
    "    #create view with 4 coronal slices next to each other to visualiza basal ganglia\n",
    "    display = plotting.plot_stat_map(volume_threshold_map, bg_img = MNI_template_used, display_mode='y', \n",
    "                                     cut_coords=[-10,-3,3,10], threshold=threshold, vmax=7, cmap='bwr',\n",
    "                                     colorbar=True, cbar_tick_format='%.1f',symmetric_cbar=True)\n",
    "    \n",
    "    if threshold:\n",
    "        rounded_thresh = round(threshold,2)\n",
    "        display.title(text=f'Thresholded z map; \"{contrast}\" contrast of {group} at {ses}, exp. fdr = .05, z_thresh = {rounded_thresh}, n = {part_count}', size=14, y=1.2)\n",
    "    else:\n",
    "        display.title(text=f'Effect size map; \"{contrast}\" contrast of {group} at {ses}, n = {part_count}', size=14, y=1.2)\n",
    "\n",
    "    return display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface_views(threshold_map, threshold, contrast, group, ses, part_count):\n",
    "        \n",
    "    right_hemi = plot_surf_stat_map(\n",
    "        hcp.mesh.inflated_right, right_cortex_data(threshold_map[hcp.struct.cortex_right], fill=0), hemi='right',\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_right, \n",
    "        vmax=7, cmap='bwr',colorbar=True,darkness=0.6,cbar_tick_format='%.1f',symmetric_cbar=True)\n",
    "    \n",
    "    right_flat = plot_surf_stat_map(\n",
    "        hcp.mesh.flat_right, right_cortex_data(threshold_map[hcp.struct.cortex_right], fill=0), hemi='right',\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_right, \n",
    "        vmax=7, cmap='bwr',colorbar=True,darkness=0.6,cbar_tick_format='%.1f',symmetric_cbar=True)\n",
    "    \n",
    "    left_hemi = plot_surf_stat_map(\n",
    "        hcp.mesh.inflated_left, left_cortex_data(threshold_map[hcp.struct.cortex_left], fill=0), hemi='left',\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_left, \n",
    "        vmax=7, cmap='bwr',colorbar=True,darkness=0.6,cbar_tick_format='%.1f',symmetric_cbar=True)\n",
    "    \n",
    "    left_flat = plot_surf_stat_map(\n",
    "        hcp.mesh.flat_left, left_cortex_data(threshold_map[hcp.struct.cortex_left], fill=0), hemi='left',\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_left, \n",
    "        vmax=7, cmap='bwr',colorbar=True,darkness=0.6,cbar_tick_format='%.1f',symmetric_cbar=True)\n",
    "    \n",
    "    return [right_hemi,right_flat,left_hemi,left_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_glass_brains(threshold_map, threshold, contrast, group, ses, part_count):\n",
    "    \n",
    "    #get any one since they're all the same\n",
    "    brain_model_axis = nibabel.load(glob.glob(f'../../../derivatives/task_analysis_surface/first_level/sub-MM*/ses-1year/task-{task}/sub-MM*_ses-1year_task-{task}_rec-unco_run-*_contrast-*_effect_size.dscalar.nii')[0]).header.get_axis(1)\n",
    "\n",
    "    #make volume from subcortical part of cifti\n",
    "    volume_threshold_map = volume_from_cifti(threshold_map, brain_model_axis)\n",
    "    \n",
    "    #display glass brain\n",
    "    display = plot_glass_brain(\n",
    "        stat_map_img = volume_threshold_map,\n",
    "        threshold=threshold,\n",
    "        colorbar=True,\n",
    "        display_mode='ortho',\n",
    "        plot_abs=False)\n",
    "    \n",
    "    rounded_thresh = round(threshold,2)\n",
    "    \n",
    "    display.title(text=f'Z scores on glass brain; \"{contrast}\" contrast of {group} at {ses}, exp. fdr = .05, z_thresh = {rounded_thresh}, n = {part_count}',\n",
    "                  size=14, y=1.2)\n",
    "    \n",
    "    return display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb96a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures(coronal_display, right_hemi, right_flat, left_hemi, left_flat, threshold, group, ses, task, contrast, part_count):\n",
    "    \n",
    "    rounded_thresh = round(threshold,2)\n",
    "    \n",
    "    #check that output does not exist\n",
    "    output = glob.glob(f'../../../derivatives/task_analysis_surface/visualization/raw_figures/group-{group}_ses-{ses}_task-{task}_contrast-{contrast}_threshold-{rounded_thresh}_display-*.png')\n",
    "    if output: #checks if list is not empty\n",
    "        print(f'At least partial group level output exists for group {group}, session {ses} and task {task}. This must be deleted for before generating new output.')\n",
    "        return \n",
    "    \n",
    "    \n",
    "    #create paths to output dir if not exist\n",
    "    derivatives_path = '../../../derivatives'\n",
    "    nilearn_output_path = os.path.join(derivatives_path, 'task_analysis_surface','visualization','raw_indiv_figures',f'task-{task}')\n",
    "    if not os.path.isdir(nilearn_output_path):\n",
    "        os.makedirs (nilearn_output_path)\n",
    "\n",
    "    \n",
    "    right_hemi.savefig(f'../../../derivatives/task_analysis_surface/visualization/raw_indiv_figures/task-{task}/group-{group}_ses-{ses}_task-{task}_contrast-{contrast}_threshold-{rounded_thresh}_n-{part_count}_display-righthemi.png')\n",
    "    \n",
    "    right_flat.savefig(f'../../../derivatives/task_analysis_surface/visualization/raw_indiv_figures/task-{task}/group-{group}_ses-{ses}_task-{task}_contrast-{contrast}_threshold-{rounded_thresh}_n-{part_count}_display-rightflat.png')\n",
    "    \n",
    "    left_hemi.savefig(f'../../../derivatives/task_analysis_surface/visualization/raw_indiv_figures/task-{task}/group-{group}_ses-{ses}_task-{task}_contrast-{contrast}_threshold-{rounded_thresh}_n-{part_count}_display-lefthemi.png')\n",
    "\n",
    "    left_flat.savefig(f'../../../derivatives/task_analysis_surface/visualization/raw_indiv_figures/task-{task}/group-{group}_ses-{ses}_task-{task}_contrast-{contrast}_threshold-{rounded_thresh}_n-{part_count}_display-leftflat.png')\n",
    " \n",
    "    coronal_display.savefig(f'../../../derivatives/task_analysis_surface/visualization/raw_indiv_figures/task-{task}/group-{group}_ses-{ses}_task-{task}_contrast-{contrast}_threshold-{rounded_thresh}_n-{part_count}_display-coronal.png')\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec456207",
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mid'\n",
    "sessions = ['baseline','1year']\n",
    "groups = ['HC','MM']\n",
    "\n",
    "\n",
    "#MNI_template_used = '../templates/tpl-MNI152NLin6Asym_res-01_desc-brain_T1w.nii.gz' #without skull\n",
    "MNI_template_used = '../templates/tpl-MNI152NLin6Asym_res-01_T1w.nii.gz' #with skull\n",
    "\n",
    "\n",
    "if task == 'mid':\n",
    "        contrasts = [ 'HiRewCue-NeuCue', #high reward anticipation -- paper A2, ABCD\n",
    "                      'LoRewCue-NeuCue', #low reward anticipation -- ABCD\n",
    "                      'RewCue-NeuCue', #combined reward anticipation -- paper A1\n",
    "                      'HiRewCue-LoRewCue', #high vs. low reward anticipation -- paper A3, ABCD\n",
    "                      'HiRewCue-Baseline', #high reward anticipation vs. baseline -- paper A4\n",
    "                      'HiLossCue-NeuCue', #high loss anticipation -- paper A5, ABCD\n",
    "                      'LoLossCue-NeuCue', #low loss anticipation -- ABCD\n",
    "                      'HiLossCue-LoLossCue', #high vs. low loss anticipation -- ABCD\n",
    "                      'HiWin-NeuHit', #high reward outcome cp. to neutral hit -- paper O6\n",
    "                      'Win-NoWin', #combined reward outcome cp. to combined reward miss -- ABCD\n",
    "                      'HiLoss-NeuHit', #high loss cp. to neutral hit -- paper O7\n",
    "                      'Loss-AvoidLoss', #combined loss cp. to combined avoid loss -- ABCD\n",
    "                     ] \n",
    "\n",
    "elif task == 'sst':\n",
    "    contrasts=['SuccStop-Go','UnsuccStop-Go','UnsuccStop-SuccStop']\n",
    "\n",
    "elif task == 'nback':\n",
    "    contrasts=['twoback-zeroback']\n",
    "\n",
    "    \n",
    "#options are age, sex, total_cudit, THC_freq_month at the moment\n",
    "#note that if total_cudit or THC_freq_month are selected, the zmap is for their slopes, not for the group average slope\n",
    "#covariates = ['age', 'sex', 'total_cudit', 'THC_freq_month']\n",
    "covariates = []\n",
    "\n",
    "#subgroup = 'dipstick_THC'\n",
    "subgroup = ''\n",
    "\n",
    "#set to true if MM participants with CUD at baseline should be excluded from the analysis\n",
    "rm_CUD = True\n",
    "\n",
    "#set this to the ones we want to include with values for fd_mean and fd_perc as keys\n",
    "desired_outliers = {'tsnr':[],'snr':[],'gsr_x':[],'gsr_y':[],'fd_mean':0.2,'fd_perc':0.3,'motion_outlier_cutoff':0.3}\n",
    "#desired_outliers = {}\n",
    "\n",
    "\n",
    "#loop through all sessions, groups, contrasts\n",
    "for ses in sessions:\n",
    "    for group in groups:        \n",
    "        for contrast in contrasts:\n",
    "            #get group average group level contrast as the output\n",
    "            contrast_outputs, part_count = get_group_average_contrast(contrast,ses,group,task,covariates,subgroup,rm_CUD,desired_outliers)\n",
    "            print(part_count)\n",
    "            \n",
    "            #account for HC not having 1year scans\n",
    "            if contrast_outputs:\n",
    "                print(f'For task contrast {contrast}, we will print the covariate-based contrasts in the following order: {contrast_outputs.keys()}:')\n",
    "\n",
    "                for slope in contrast_outputs.keys():\n",
    "                    \n",
    "                    contrast_output = contrast_outputs[slope]\n",
    "                \n",
    "                    #compute FDR-corrected z-score thresholdmap\n",
    "                    #note that this does not have a cluster threshold\n",
    "                    threshold_map, threshold = fdr_threshold_map(contrast_output.z_score())\n",
    "\n",
    "                    #toggle this depending on if pngs should get saved of the plots\n",
    "                    if slope == 'group_average' and subgroup == '':\n",
    "                                            \n",
    "                        #thresholded displays\n",
    "                        coronal_display = plot_coronal_slices(threshold_map, threshold, MNI_template_used, contrast, group, ses, part_count)\n",
    "                        right_hemi,right_flat,left_hemi,left_flat = plot_surface_views(threshold_map, threshold, contrast, group, ses, part_count)\n",
    "\n",
    "#                         #unthresholded displays\n",
    "#                         coronal_display = plot_coronal_slices(contrast_output.z_score(), 0, MNI_template_used, contrast, group, ses, part_count)\n",
    "#                         right_hemi,right_flat,left_hemi,left_flat = plot_surface_views(contrast_output.z_score(), 0, contrast, group, ses, part_count)\n",
    "\n",
    "                        save_figures(coronal_display,right_hemi,right_flat,left_hemi,left_flat,threshold, group, ses, task, contrast,part_count)\n",
    "\n",
    "\n",
    "#toggle this depending on if pngs should be shown below\n",
    "#plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5db3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430ec73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940d848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group difference model for control vs. MM at baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a26c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_group_difference_contrast(contrast,ses,groups,task,covariates,subgroup,rm_CUD,desired_outliers):\n",
    "    \n",
    "    #get individual effect size maps and a design matrix per group\n",
    "    effect_size_maps=[] #list of both groups effect size maps\n",
    "    group_sizes=[] #list of the two group sizes\n",
    "    group_design_matrices=[] #get separate design matrices per group that will be combined\n",
    "    \n",
    "    for group in groups:\n",
    "        \n",
    "        excluded_subs_by_group_runs_dict = get_excluded_runs(desired_outliers,contrast,ses,group,task)\n",
    "        effect_size_maps_by_group, all_subs = get_effect_size_maps(contrast,ses,group,task,excluded_subs_by_group_runs_dict)\n",
    "\n",
    "        #remove participants with CUD at baseline (only applies to MM)\n",
    "        if rm_CUD:\n",
    "            effect_size_maps_by_group, all_subs = rm_CUD_baseline(effect_size_maps_by_group, all_subs)\n",
    "\n",
    "        #note that subject formatting is changed slightly to match that of the non_imaging_data csv file\n",
    "        #need these to make design matrix\n",
    "        subs_csv = ['_'.join([s for s in re.split(r'(MM|HC)', sub) if s]) for sub in all_subs]\n",
    "    \n",
    "        if subgroup:\n",
    "            effect_size_maps_by_group, subs_csv, all_subs = select_subgroup(subgroup, effect_size_maps_by_group, subs_csv, all_subs)\n",
    "\n",
    "        effect_size_signals_by_group = [nimg.load_img(dscalar).get_fdata(dtype='f4') for dscalar in effect_size_maps_by_group]\n",
    "        effect_size_signals_combined_by_group = np.vstack(effect_size_signals_by_group)\n",
    "        effect_size_maps.append(effect_size_signals_combined_by_group)\n",
    "\n",
    "        group_sizes.append(len(effect_size_maps_by_group))\n",
    "        \n",
    "        #create design matrix per group\n",
    "        #note that the columns that need to stay separate when stacking the design matrices later get renamed\n",
    "        indv_design_matrix = create_group_design_matrix(subs_csv,group,ses,covariates)\n",
    "\n",
    "        indv_design_matrix.rename({'group_average': f'group_average_{group}'}, axis='columns',inplace=True)\n",
    "        group_design_matrices.append(indv_design_matrix)\n",
    "    \n",
    "    \n",
    "    #stack effect size maps of all participants from both groups\n",
    "    effect_size_maps_stacked = np.vstack(effect_size_maps)\n",
    "    \n",
    "    \n",
    "    #stack design matrices of the two groups to make one large design matrix\n",
    "    #note again that group_average or Male&Female columns, which were renamed, will not get stacked\n",
    "    #thus, these will partially contain NaNs, which will be replaced by 0s\n",
    "    group_design_matrix = pd.concat(group_design_matrices).replace(np. nan,0) \n",
    "\n",
    "    print(group_design_matrix)\n",
    "    \n",
    "    \n",
    "    #set up and run glm\n",
    "    #note that we are not smoothing with nilearn since the data was smoothed using the HCP pipeline\n",
    "    labels, estimates = run_glm(effect_size_maps_stacked, group_design_matrix.values, noise_model='ols', \n",
    "                                n_jobs=-2, verbose=0)\n",
    "\n",
    "    \n",
    "    #make list of possible contrasts: HC only, MM only, MM-HC\n",
    "    contrast_matrix = np.eye(group_design_matrix.shape[1])\n",
    "    grp_contrasts = dict([(column, contrast_matrix[i])\n",
    "                      for i, column in enumerate(group_design_matrix.columns)])\n",
    "    \n",
    "\n",
    "    grp_contrasts['group_average'] = grp_contrasts['group_average_MM']-grp_contrasts['group_average_HC']\n",
    "        \n",
    "    print(grp_contrasts)\n",
    "    \n",
    "    \n",
    "    #compute all the group contrasts\n",
    "    contrast_outputs = {}\n",
    "        \n",
    "    for key in grp_contrasts.keys():\n",
    "        if 'group_average_' not in key:\n",
    "            contrast_output = compute_contrast(labels, estimates, grp_contrasts[key], contrast_type='t')\n",
    "            contrast_outputs[key] = contrast_output\n",
    "    \n",
    "        \n",
    "    #for printing the covariates\n",
    "    if covariates == '':\n",
    "        covariates = 'nothing'\n",
    "        \n",
    "    print('Processing done of the baseline difference of MM and HC for: '+' '.join([contrast,task]))\n",
    "    print(f'Controlled for {covariates}')\n",
    "\n",
    "    \n",
    "    return (contrast_outputs,(group_sizes[0],group_sizes[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90339814",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=['MM','HC']\n",
    "ses='baseline'\n",
    "task='mid'\n",
    "space='MNI152NLin6Asym' #change if desired\n",
    "covariates_list = [[],['sex'],['age'],['THC_freq_month'],['total_cudit'],['age','sex'],['THC_freq_month','age','sex'],['total_cudit','age','sex'] ]\n",
    "MNI_template_used = nibabel.load('../templates/tpl-MNI152NLin6Asym_res-02_desc-brain_T1w.nii.gz')\n",
    "\n",
    "\n",
    "if task == 'mid':\n",
    "        contrasts = [ 'HiRewCue-NeuCue', #high reward anticipation -- paper A2, ABCD\n",
    "                      'LoRewCue-NeuCue', #low reward anticipation -- ABCD\n",
    "                      'RewCue-NeuCue', #combined reward anticipation -- paper A1\n",
    "                      'HiRewCue-LoRewCue', #high vs. low reward anticipation -- paper A3, ABCD\n",
    "                      'HiRewCue-Baseline', #high reward anticipation vs. baseline -- paper A4\n",
    "                      'HiLossCue-NeuCue', #high loss anticipation -- paper A5, ABCD\n",
    "                      'LoLossCue-NeuCue', #low loss anticipation -- ABCD\n",
    "                      'HiLossCue-LoLossCue', #high vs. low loss anticipation -- ABCD\n",
    "                      'HiWin-NeuHit', #high reward outcome cp. to neutral hit -- paper O6\n",
    "                      'Win-NoWin', #combined reward outcome cp. to combined reward miss -- ABCD\n",
    "                      'HiLoss-NeuHit', #high loss cp. to neutral hit -- paper O7\n",
    "                      'Loss-AvoidLoss', #combined loss cp. to combined avoid loss -- ABCD\n",
    "                     ] \n",
    "\n",
    "elif task == 'sst':\n",
    "    contrasts=['SuccStop-Go','UnsuccStop-Go','UnsuccStop-SuccStop']\n",
    "\n",
    "elif task == 'nback':\n",
    "    contrasts=['twoback-zeroback']\n",
    "\n",
    "    \n",
    "#since we're comparing to HC, setting to only MCC subgroup doesn't make sense    \n",
    "subgroup = ''\n",
    "\n",
    "\n",
    "#set to true if MM participants with CUD at baseline should be excluded from the analysis\n",
    "rm_CUD = True\n",
    "\n",
    "#set this to the ones we want to include with values for fd_mean and fd_perc as keys\n",
    "desired_outliers = {'tsnr':[],'snr':[],'gsr_x':[],'gsr_y':[],'fd_mean':0.2,'fd_perc':0.3,'motion_outlier_cutoff':0.3}\n",
    "#desired_outliers = {}\n",
    "\n",
    "\n",
    "#TO: loop through all contrasts\n",
    "for covariates in covariates_list:\n",
    "    for contrast in contrasts:\n",
    "        \n",
    "        contrast_outputs, part_count = get_baseline_group_difference_contrast(contrast,ses,groups,task,covariates,subgroup,rm_CUD,desired_outliers)\n",
    "        \n",
    "        print(covariates)\n",
    "        print(f'We will print the covariate-based contrasts in the following order: {contrast_outputs.keys()}:')\n",
    "\n",
    "        for key in contrast_outputs.keys():\n",
    "            \n",
    "            contrast_output = contrast_outputs[key]\n",
    "\n",
    "            #calculate fdr-thresholded map\n",
    "            threshold_map, threshold = fdr_threshold_map(contrast_output.z_score())\n",
    "            \n",
    "            if not np.isinf(threshold):\n",
    "                \n",
    "            #make visualizations\n",
    "\n",
    "                fig, ax = plt.subplots()\n",
    "                fig.text(0.5, 0.5, f\"{contrast}, controlled for: {covariates}, showing: {key}\", ha='center', va='center', fontsize=16)\n",
    "                ax.axis('off')\n",
    "\n",
    "                coronal_display = plot_coronal_slices(threshold_map, threshold, MNI_template_used, contrast, 'MM minus HC', ses, part_count)\n",
    "                glass_brain = plot_glass_brains(threshold_map, threshold, contrast, 'MM minus HC', ses, part_count)\n",
    "\n",
    "                right_hemi,right_flat,left_hemi,left_flat = plot_surface_views(threshold_map, threshold, contrast, 'MM minus HC', ses, part_count)\n",
    "\n",
    "                print(threshold)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6c96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef60d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2293d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAIRED group difference model for MM at 1year vs. baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9eb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MM_paired_group_difference_contrast(contrast,sessions,group,task,covariates,subgroup,rm_CUD,desired_outliers):\n",
    "        \n",
    "    #get individual effect size maps by subjects\n",
    "    effect_size_maps_by_ses = [] #list of two dictionaries, one per ses, both with subjects as keys and their effect size paths as values\n",
    "    \n",
    "    for ses in sessions:\n",
    "        excluded_subs_runs_dict = get_excluded_runs(desired_outliers,contrast,ses,group,task)\n",
    "        \n",
    "        effect_size_maps, all_subs = get_effect_size_maps(contrast,ses,group,task,excluded_subs_runs_dict)\n",
    "        #remove participants with CUD at baseline (only applies to MM)\n",
    "        if rm_CUD:\n",
    "            effect_size_maps, all_subs = rm_CUD_baseline(effect_size_maps, all_subs)\n",
    "        effect_size_by_subj_dict = dict(zip(all_subs, effect_size_maps))\n",
    "        effect_size_maps_by_ses.append(effect_size_by_subj_dict)\n",
    "\n",
    "    #find subjects with both baseline and 1year scans\n",
    "    subjs_with_both_ses = list(set(effect_size_maps_by_ses[0].keys()).intersection(set(effect_size_maps_by_ses[1].keys())))\n",
    "    \n",
    "    #subtract baseline from 1year scans\n",
    "    #save as list of nii effect size maps\n",
    "    effect_size_arrays = []\n",
    "    for sub in subjs_with_both_ses:\n",
    "        effect_size_map_0 = effect_size_maps_by_ses[0][sub]\n",
    "        effect_size_arr_0 = nimg.load_img(effect_size_map_0).get_fdata(dtype='f4')\n",
    "        effect_size_map_1 = effect_size_maps_by_ses[1][sub]\n",
    "        effect_size_arr_1 = nimg.load_img(effect_size_map_1).get_fdata(dtype='f4')\n",
    "        diff_arr = effect_size_arr_0 - effect_size_arr_1\n",
    "        effect_size_arrays.append(diff_arr)\n",
    "\n",
    "    #rewrite subject names so they match the csv file\n",
    "    subs_csv = ['_'.join([s for s in re.split(r'(MM|HC)', sub) if s]) for sub in subjs_with_both_ses]\n",
    "    \n",
    "    if subgroup:\n",
    "        effect_size_arrays, subs_csv, subjs_with_both_ses = select_subgroup(subgroup, effect_size_arrays, subs_csv, subjs_with_both_ses)\n",
    "    \n",
    "    part_count = len(subjs_with_both_ses)\n",
    "    \n",
    "    #get design matrix for the group\n",
    "    #if sex present, will include Male/Female; otherwise will include a column of all 1s called group average\n",
    "    group_design_matrix = create_group_design_matrix(subs_csv,group,ses,covariates)\n",
    "\n",
    "\n",
    "    effect_size_arrays_combined = np.vstack(effect_size_arrays)\n",
    "\n",
    "\n",
    "    #set up and run glm\n",
    "    #note that we are not smoothing with nilearn since the data was smoothed using the HCP pipeline\n",
    "    labels, estimates = run_glm(effect_size_arrays_combined, group_design_matrix.values, noise_model='ols', \n",
    "                                n_jobs=-2, verbose=0)\n",
    "  \n",
    "\n",
    "    #define contrast matrix to include single column of 1s \n",
    "    contrast_matrix = np.eye(group_design_matrix.shape[1])\n",
    "    grp_contrasts = dict([(column, contrast_matrix[i])\n",
    "                  for i, column in enumerate(group_design_matrix.columns)])\n",
    "    \n",
    "    print(grp_contrasts)\n",
    "    \n",
    "    #compute all the group contrasts\n",
    "    contrast_outputs = {}\n",
    "        \n",
    "    for key in grp_contrasts.keys():\n",
    "        contrast_output = compute_contrast(labels, estimates, grp_contrasts[key], contrast_type='t')\n",
    "        contrast_outputs[key] = contrast_output\n",
    "\n",
    "    \n",
    "    #for printing the covariates\n",
    "    if covariates == '':\n",
    "        covariates = 'nothing'\n",
    "        \n",
    "    print('Processing done for: '+' '.join([contrast,ses,group,task]))\n",
    "    print(f'Controlled for {covariates}')\n",
    "\n",
    "    return (contrast_outputs, part_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ac932",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions=['1year','baseline']\n",
    "task='mid'\n",
    "group='MM'\n",
    "covariates_list = [[],['sex'],['age'],['THC_freq_month'],['total_cudit'],['age','sex'],['THC_freq_month','age','sex'],['total_cudit','age','sex']]\n",
    "#covariates = [[]]\n",
    "space='MNI152NLin6Asym' #change if desired\n",
    "MNI_template_used = nibabel.load('../templates/tpl-MNI152NLin6Asym_res-01_desc-brain_T1w.nii.gz')\n",
    "\n",
    "\n",
    "if task == 'mid':\n",
    "        contrasts = [ 'HiRewCue-NeuCue', #high reward anticipation -- paper A2, ABCD\n",
    "                      'LoRewCue-NeuCue', #low reward anticipation -- ABCD\n",
    "                      'RewCue-NeuCue', #combined reward anticipation -- paper A1\n",
    "                      'HiRewCue-LoRewCue', #high vs. low reward anticipation -- paper A3, ABCD\n",
    "                      'HiRewCue-Baseline', #high reward anticipation vs. baseline -- paper A4\n",
    "                      'HiLossCue-NeuCue', #high loss anticipation -- paper A5, ABCD\n",
    "                      'LoLossCue-NeuCue', #low loss anticipation -- ABCD\n",
    "                      'HiLossCue-LoLossCue', #high vs. low loss anticipation -- ABCD\n",
    "                      'HiWin-NeuHit', #high reward outcome cp. to neutral hit -- paper O6\n",
    "                      'Win-NoWin', #combined reward outcome cp. to combined reward miss -- ABCD\n",
    "                      'HiLoss-NeuHit', #high loss cp. to neutral hit -- paper O7\n",
    "                      'Loss-AvoidLoss', #combined loss cp. to combined avoid loss -- ABCD\n",
    "                     ] \n",
    "\n",
    "elif task == 'sst':\n",
    "    contrasts=['SuccStop-Go','UnsuccStop-Go','UnsuccStop-SuccStop']\n",
    "\n",
    "elif task == 'nback':\n",
    "    contrasts=['twoback-zeroback']\n",
    "    \n",
    "    \n",
    "subgroup = ''\n",
    "#subgroup = 'dipstick_THC'\n",
    "\n",
    "#set to true if MM participants with CUD at baseline should be excluded from the analysis\n",
    "rm_CUD = True\n",
    "\n",
    "#set this to the ones we want to include with values for fd_mean and fd_perc as keys\n",
    "desired_outliers = {'tsnr':[],'snr':[],'gsr_x':[],'gsr_y':[],'fd_mean':0.2,'fd_perc':0.3,'motion_outlier_cutoff':0.3}\n",
    "#desired_outliers = {}\n",
    "\n",
    "for covariates in covariates_list:\n",
    "    for contrast in contrasts:\n",
    "        contrast_outputs, part_count = get_MM_paired_group_difference_contrast(contrast,sessions,group,task,covariates,subgroup,rm_CUD,desired_outliers)\n",
    "            \n",
    "        for key in contrast_outputs.keys():\n",
    "            \n",
    "            contrast_output = contrast_outputs[key]\n",
    "\n",
    "            #calculate fdr-thresholded map\n",
    "            threshold_map, threshold = fdr_threshold_map(contrast_output.z_score())\n",
    "            \n",
    "            if True:\n",
    "            #if not np.isinf(threshold):\n",
    "                \n",
    "                #make visualizations\n",
    "\n",
    "                fig, ax = plt.subplots()\n",
    "                fig.text(0.5, 0.5, f\"{contrast}, controlled for: {covariates}, showing: {key}\", ha='center', va='center', fontsize=16)\n",
    "                ax.axis('off')\n",
    "\n",
    "                coronal_display = plot_coronal_slices(threshold_map, threshold, MNI_template_used, contrast, group, '1year minus baseline', part_count)\n",
    "                glass_brain = plot_glass_brains(threshold_map, threshold, contrast, group, '1year minus baseline', part_count)\n",
    "\n",
    "                right_hemi,right_flat,left_hemi,left_flat = plot_surface_views(threshold_map, threshold, contrast, group, '1year minus baseline', part_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
