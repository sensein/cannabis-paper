{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nibabel\n",
    "import nilearn\n",
    "\n",
    "from nilearn import image as nimg\n",
    "from nilearn import plotting \n",
    "from nilearn.plotting import plot_glass_brain\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.masking import intersect_masks\n",
    "from nilearn.image import math_img\n",
    "from nilearn.glm import threshold_stats_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 3 group means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_average_contrast(contrast,ses,group,task,covariates,subgroup,rm_CUD,desired_outliers):\n",
    "    print('Now processing: '+ ' '.join([contrast,ses,group,task]))\n",
    "    space='MNI152NLin6Asym' \n",
    "    \n",
    "    #check if mriqc summary exists and find runs that need to be excluded based on desired outlier cutoffs\n",
    "    if desired_outliers:\n",
    "        if len(glob.glob(f'../../../derivatives/mriqc_summaries/group-{group}_ses-{ses}_task-{task}_rec-unco.tsv'))==0:\n",
    "            print('No mriqc summary found. Please create this before requesting to remove outliers.')\n",
    "            return (None, None, None, None)\n",
    "\n",
    "    excluded_subs_runs_dict = get_excluded_runs(desired_outliers,contrast,ses,group,task)\n",
    "\n",
    "    effect_size_maps, all_subs = get_effect_size_maps(contrast,ses,group,task,excluded_subs_runs_dict)\n",
    "    \n",
    "    \n",
    "    #remove participants with CUD at baseline (only applies to MM)\n",
    "    if rm_CUD:\n",
    "        effect_size_maps, all_subs = rm_CUD_baseline(effect_size_maps, all_subs)\n",
    "\n",
    "        \n",
    "    #ensure we have data\n",
    "    if not effect_size_maps:\n",
    "        print('No effect size maps, so cannot generate group level output for: '+' '.join([contrast,ses,group,task]))\n",
    "        return (None, None, None, None)\n",
    "    \n",
    "    #note that subject formatting is changed slightly to match that of the non_imaging_data csv file\n",
    "    #need these to make design matrix\n",
    "    subs_csv = ['_'.join([s for s in re.split(r'(MM|HC)', sub) if s]) for sub in all_subs]\n",
    "    \n",
    "    \n",
    "    if subgroup:\n",
    "        effect_size_maps, subs_csv, all_subs = select_subgroup(subgroup, effect_size_maps, subs_csv, all_subs)\n",
    "\n",
    "    \n",
    "    #needed for printing total later\n",
    "    part_count = len(subs_csv)\n",
    "    \n",
    "    #create design matrix including all covariates and a way to encode group average (either as col of 1s or with Male/Female)\n",
    "    group_design_matrix = create_group_design_matrix(subs_csv,group,ses,covariates)\n",
    "    display(group_design_matrix)\n",
    "    \n",
    "    #grab masks needed\n",
    "    masks = []\n",
    "    for sub in all_subs:\n",
    "        masks+=glob.glob(f'../../../derivatives/ses-{ses}/sub-{sub}*/ses-{ses}/func/sub-{sub}*task-{task}*rec-unco*run-*_*{space}*brain_mask.nii.gz')\n",
    "\n",
    "    #make intersection of the masks\n",
    "    #threshold=1 corresponds to keeping the intersection of all masks, whereas threshold=0 is the union of all masks\n",
    "    mask = intersect_masks(masks, threshold=1, connected=True)\n",
    "\n",
    "\n",
    "    #define model specs\n",
    "    second_level_model = SecondLevelModel(mask_img=mask, target_affine=None, target_shape=None, \n",
    "                                          smoothing_fwhm=None, memory_level=1, \n",
    "                                          verbose=0, n_jobs=-2, minimize_memory=False)\n",
    "\n",
    "    #fit model\n",
    "    second_level_model = second_level_model.fit(effect_size_maps, design_matrix=group_design_matrix,)\n",
    "\n",
    "    \n",
    "    #define contrast matrix \n",
    "    contrast_matrix = np.eye(group_design_matrix.shape[1])\n",
    "    grp_contrasts = dict([(column, contrast_matrix[i])\n",
    "                      for i, column in enumerate(group_design_matrix.columns)])\n",
    "\n",
    "    print(grp_contrasts)\n",
    "\n",
    "    \n",
    "    contrast_outputs = {}\n",
    "    \n",
    "    for key in grp_contrasts.keys():\n",
    "        #compute the contrasts\n",
    "        #pick stat_type none because it automatically selects t vs. F test \n",
    "        contrast_output = second_level_model.compute_contrast(second_level_contrast=grp_contrasts[key], \n",
    "                                                              second_level_stat_type=None, output_type='all')\n",
    "        contrast_outputs[key] = contrast_output\n",
    "\n",
    "        \n",
    "    #for printing the covariates\n",
    "    if covariates == '':\n",
    "        covariates = 'nothing'\n",
    "        \n",
    "    print('Processing done for: '+' '.join([contrast,ses,group,task]))\n",
    "    print(f'Controlled for {covariates}')\n",
    "    \n",
    "    return (contrast_outputs, mask, part_count, subs_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae962fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excluded_runs(desired_outliers,contrast,ses,group,task):\n",
    "    \n",
    "    #empty list and dict to save runs to be excluded\n",
    "    excluded_runs=[]\n",
    "    excluded_runs_dict={}\n",
    "    \n",
    "    if len(desired_outliers.keys())==0:\n",
    "        return excluded_runs_dict\n",
    "    \n",
    "    #read in cutoffs based on Q1-1.5*IQR and Q3+1.5*IQR based on all tasks and groups (not just current) for this session\n",
    "    iqr_cutoffs_df = pd.read_csv(f'../../../derivatives/mriqc_summaries/iqr_cutoffs_ses-{ses}_rec-unco.tsv', low_memory=False, sep='\\t')\n",
    "    \n",
    "    #read in tsv file with mriqc summary for current task, ses, group combi\n",
    "    mriqc_summary = pd.read_csv(f'../../../derivatives/mriqc_summaries/group-{group}_ses-{ses}_task-{task}_rec-unco.tsv', low_memory=False, sep='\\t')\n",
    "    \n",
    "    #possible outlier types that use IQR-based cutoff: tsnr, snr, gsr_x, gsr_y\n",
    "    #exclude runs based on these if in desired_outliers list\n",
    "    for outlier_type in ['tsnr', 'snr', 'gsr_x', 'gsr_y']:\n",
    "        if outlier_type in desired_outliers.keys():\n",
    "            #get upper and lower cutoffs from iqr summaries df\n",
    "            lower_bound = iqr_cutoffs_df[outlier_type].iloc[0]\n",
    "            upper_bound = iqr_cutoffs_df[outlier_type].iloc[1]\n",
    "            \n",
    "            #find subs_runs combinations outside of cutoffs \n",
    "            above_subs_runs = mriqc_summary['subs_runs'][mriqc_summary[outlier_type]>=upper_bound].tolist()\n",
    "            below_subs_runs = mriqc_summary['subs_runs'][mriqc_summary[outlier_type]<=lower_bound].tolist()\n",
    "            subs_runs = above_subs_runs+below_subs_runs\n",
    "            excluded_runs+=subs_runs\n",
    "            \n",
    "    if 'fd_mean' in desired_outliers.keys():\n",
    "        above_fd_mean_runs = mriqc_summary['subs_runs'][mriqc_summary['fd_mean']>desired_outliers['fd_mean']].tolist()\n",
    "        if 'fd_perc' in desired_outliers:\n",
    "            below_fd_perc_runs = set(mriqc_summary['subs_runs'][mriqc_summary['fd_perc']<desired_outliers['fd_perc']].tolist())\n",
    "            above_fd_mean_runs = set(above_fd_mean_runs)\n",
    "            above_fd_mean_runs = list(above_fd_mean_runs.difference(below_fd_perc_runs))\n",
    "        excluded_runs+=above_fd_mean_runs\n",
    "    \n",
    "    if 'motion_outlier_cutoff' in desired_outliers.keys():\n",
    "        perc_cutoff =  desired_outliers['motion_outlier_cutoff']\n",
    "        \n",
    "        if task == 'nback':\n",
    "            n_scans = 278\n",
    "        elif task == 'mid':\n",
    "            n_scans = 215\n",
    "        elif task == 'sst':\n",
    "            n_scans = 257\n",
    "        \n",
    "        motion_outlier_df = pd.read_csv(f'../../../derivatives/motion_outlier_counts/{task}_motion_outlier_count.csv', low_memory=False)\n",
    "        motion_outlier_above_cutoff_runs = motion_outlier_df['subs_runs'][motion_outlier_df['motion_outlier_count']>(n_scans*perc_cutoff)].tolist()\n",
    "    \n",
    "        excluded_runs+=motion_outlier_above_cutoff_runs\n",
    "    \n",
    "    \n",
    "    #remove duplicates\n",
    "    excluded_runs=list(set(excluded_runs))\n",
    "    \n",
    "    #turn into dict\n",
    "    for sub_run in excluded_runs:\n",
    "        sub = sub_run.split('_')[0]\n",
    "        run = sub_run.split('_')[1]\n",
    "        if sub in excluded_runs_dict.keys():\n",
    "            excluded_runs_dict[sub].append(run)\n",
    "        else:\n",
    "            excluded_runs_dict[sub]=[run]\n",
    "    \n",
    "    return excluded_runs_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ee165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_CUD_baseline(effect_size_maps, all_subs):\n",
    "    \n",
    "    #subs to be excluded (only MM) because they had cannabis use disorder at baseline (exclusion criterium)\n",
    "    excluded_subs = ['MM014','MM188','MM197','MM217','MM228','MM239','MM241']\n",
    "    \n",
    "    #get only effect size maps that aren't those of any of the excluded subjects\n",
    "    final_effect_size_maps = [path for path in effect_size_maps if path.split('/sub-')[1].split('/')[0] not in excluded_subs]\n",
    "\n",
    "    #get only subjects that aren't those of any of the excluded subjects\n",
    "    final_all_subs = [sub for sub in all_subs if sub not in excluded_subs]\n",
    "\n",
    "    return final_effect_size_maps, final_all_subs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subgroup(subgroup, effect_size_maps, subs_csv, subs):\n",
    "    non_img_data = pd.read_csv(f\"../../../sourcedata/non_imaging_data/MMJ-Processed_data-2022_05_27-13_58-6858bbe.csv\",low_memory=False)\n",
    "    final_subs = []\n",
    "    final_subs_csv = []\n",
    "    final_effect_size_maps = []\n",
    "    if subgroup == 'dipstick_THC':\n",
    "        dipstick_THC_results_dict = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'One year'].groupby('IDS.CHR.Subject')['URN.LGC.THC_present'].agg(\"first\").to_dict()\n",
    "        for i in range(len(effect_size_maps)):\n",
    "            if subs_csv[i] in dipstick_THC_results_dict.keys():\n",
    "                if dipstick_THC_results_dict[subs_csv[i]]:\n",
    "                    final_subs.append(subs[i])\n",
    "                    final_subs_csv.append(subs_csv[i])\n",
    "                    final_effect_size_maps.append(effect_size_maps[i])\n",
    "                \n",
    "    return (final_effect_size_maps, final_subs_csv, final_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size_maps(contrast,ses,group,task,excluded_subs_runs_dict):\n",
    "        \n",
    "    #get individual effect size maps\n",
    "    #get all second-level contrasts that exist\n",
    "    second_level_effect_size_maps = glob.glob(f'../../../derivatives/task_analysis_volume/second_level/sub-{group}*/ses-{ses}/task-{task}/sub-{group}*_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_effect_size_fx.nii.gz')\n",
    "    second_level_subs = [path.split('/sub-')[1].split('/')[0] for path in second_level_effect_size_maps if path]\n",
    "    second_level_dict = dict(zip(second_level_subs, second_level_effect_size_maps))\n",
    "\n",
    "    #remove any second level contrasts who have at least one run that needs to be excluded \n",
    "    for sub in excluded_subs_runs_dict.keys():\n",
    "        if sub in second_level_dict.keys():\n",
    "            second_level_dict.pop(sub)\n",
    "    \n",
    "    second_level_subs = list(second_level_dict.keys())\n",
    "    second_level_effect_size_maps = list(second_level_dict.values())\n",
    "    \n",
    "    print(f'second level data subs count: {len(second_level_subs)}')\n",
    "    \n",
    "    #get first-level contrasts for the remaining subjects\n",
    "    all_first_level_effect_size_maps = glob.glob(f'../../../derivatives/task_analysis_volume/first_level/sub-{group}*/ses-{ses}/task-{task}/sub-{group}*_ses-{ses}_task-{task}_rec-unco_run-*_contrast-{contrast}_effect_size.nii.gz')\n",
    "    first_level_effect_size_maps = [path for path in all_first_level_effect_size_maps if path.split('/sub-')[1].split('/')[0] not in second_level_subs]\n",
    "    first_level_subs = [path.split('/sub-')[1].split('/')[0] for path in first_level_effect_size_maps if path]\n",
    "    first_level_runs = [path.split('run-')[1].split('_')[0] for path in first_level_effect_size_maps if path]\n",
    "\n",
    "    #need to work with sub_run keys since otherwise we'd have duplicate keys\n",
    "    first_level_subs_runs = ['_'.join(x) for x in zip(first_level_subs,first_level_runs)]\n",
    "    first_level_dict = dict(zip(first_level_subs_runs, first_level_effect_size_maps))\n",
    "\n",
    "    for sub,run_list in excluded_subs_runs_dict.items():\n",
    "        for run in run_list:\n",
    "            sub_run = f'{sub}_{run}'\n",
    "            if sub_run in first_level_dict.keys():\n",
    "                first_level_dict.pop(sub_run)\n",
    "    \n",
    "    first_level_subs = [sub_run.split('_')[0] for sub_run in first_level_dict.keys()]\n",
    "    first_level_effect_size_maps = list(first_level_dict.values())\n",
    "    \n",
    "    print(f'first level data subs count: {len(first_level_subs)}')\n",
    "    \n",
    "    #combine the effect_size_maps into one list and the subjects into one list\n",
    "    effect_size_maps = second_level_effect_size_maps + first_level_effect_size_maps\n",
    "    \n",
    "    all_subs = second_level_subs + first_level_subs\n",
    "    \n",
    "    print(f'all subs count: {len(all_subs)}')\n",
    "    \n",
    "    return (effect_size_maps, all_subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_design_matrix(subs,group,ses,covariates):\n",
    "    \n",
    "    df_subs = pd.DataFrame(subs,columns=['subs'])\n",
    "    non_img_data = pd.read_csv(f\"../../../sourcedata/non_imaging_data/MMJ-Processed_data-2022_05_27-13_58-6858bbe.csv\",low_memory=False)\n",
    "    \n",
    "    #create design matrix with 1 column of 1s and as many rows as there are subjects\n",
    "    group_design_matrix = pd.DataFrame([1] * len(subs), columns=['group_average'],)\n",
    "    \n",
    "    if 'sex' in covariates:\n",
    "        #add columns for male and female, that will then be combined to create the group average \n",
    "        grouped_sex = non_img_data.groupby(\"IDS.CHR.Subject\")[\"SBJ.CHR.Sex\"].agg(\"first\")\n",
    "        dict_sex = grouped_sex.to_dict()\n",
    "        df_subs['sex'] = df_subs['subs'].map(dict_sex)\n",
    "        dummy_df = pd.get_dummies(df_subs['sex'])\n",
    "        group_design_matrix['sex'] = dummy_df['Male']\n",
    "    \n",
    "    #numericals can be added directly\n",
    "    #mean center numerical values like age and CUDIT!\n",
    "\n",
    "    if 'age' in covariates:\n",
    "        #numerical can be added directly\n",
    "        #mean center numerical value\n",
    "        grouped_age = non_img_data.groupby(\"IDS.CHR.Subject\")[\"SBJ.INT.Age\"].agg(\"first\")\n",
    "        dict_age = grouped_age.to_dict()\n",
    "        group_design_matrix['age'] = df_subs['subs'].map(dict_age)\n",
    "        group_design_matrix['age'] = group_design_matrix['age'] - group_design_matrix['age'].mean()\n",
    "    \n",
    "    if 'total_cudit' in covariates:\n",
    "        if group == 'HC': \n",
    "            grouped_HC_baseline_cudit = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Screening'].groupby('IDS.CHR.Subject')['INV.INT.CUDIT.Summed_score'].agg(\"first\")\n",
    "            dict_HC_baseline_cudit = grouped_HC_baseline_cudit.to_dict()\n",
    "            group_design_matrix['total_cudit'] = df_subs['subs'].map(dict_HC_baseline_cudit)\n",
    "            group_design_matrix['total_cudit'] = group_design_matrix['total_cudit'] - group_design_matrix['total_cudit'].mean()\n",
    "\n",
    "        else:\n",
    "            if ses == 'baseline':\n",
    "                dict_MM_baseline_cudit = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Baseline'].groupby('IDS.CHR.Subject')['INV.INT.CUDIT.Summed_score'].agg(\"first\").to_dict()\n",
    "                group_design_matrix['total_cudit'] = df_subs['subs'].map(dict_MM_baseline_cudit)\n",
    "                group_design_matrix['total_cudit'] = group_design_matrix['total_cudit'] - group_design_matrix['total_cudit'].mean()\n",
    "                #needed because MM_141 is nan, so replacing with 0, which is the mean\n",
    "                group_design_matrix['total_cudit'].fillna(0, inplace=True)\n",
    "            else:\n",
    "                dict_MM_1year_cudit = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'One year'].groupby('IDS.CHR.Subject')['INV.INT.CUDIT.Summed_score'].agg(\"first\").to_dict()\n",
    "                group_design_matrix['total_cudit'] = df_subs['subs'].map(dict_MM_1year_cudit)\n",
    "                group_design_matrix['total_cudit'] = group_design_matrix['total_cudit'] - group_design_matrix['total_cudit'].mean()\n",
    "    \n",
    "    #encode the correspondence \n",
    "    freq_dict = {'Once or more per day':7,\n",
    "        '5-6 days a week':6,\n",
    "        '3-4 days a week':5,\n",
    "        '1-2 days a week':4,\n",
    "        'Less than once a week':3,\n",
    "        'Less than once every two weeks':2,\n",
    "        'Less than once a month':1,\n",
    "        None:0}\n",
    "    \n",
    "    if 'THC_freq_month' in covariates:\n",
    "        if group == 'HC':\n",
    "            #results from screening visit (using this for consistency since CUDIT-R was also collected at screening visit)\n",
    "            dict_HC_screening_THC = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Screening'].groupby('IDS.CHR.Subject')['TLF.CHR.THC.Frequency_in_month'].agg(\"last\").to_dict()\n",
    "            dict_HC_screening_THC_num = {k:freq_dict[v] for k,v in dict_HC_screening_THC.items()}\n",
    "            group_design_matrix['THC_freq_month'] = df_subs['subs'].map(dict_HC_screening_THC_num)\n",
    "            group_design_matrix['THC_freq_month'] = group_design_matrix['THC_freq_month'] - group_design_matrix['THC_freq_month'].mean()\n",
    "        else:\n",
    "            if ses == 'baseline':     \n",
    "                #results from MRI visit (using this for consistency since CUDIT-R was also collected at MRI visit)\n",
    "                dict_MM_MRIvisit_THC = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'Baseline'].groupby('IDS.CHR.Subject')['TLF.CHR.THC.Frequency_in_month'].agg(\"first\").to_dict()\n",
    "                dict_MM_MRIvisit_THC_num = {k:freq_dict[v] for k,v in dict_MM_MRIvisit_THC.items()}\n",
    "                group_design_matrix['THC_freq_month'] = df_subs['subs'].map(dict_MM_MRIvisit_THC_num)\n",
    "                group_design_matrix['THC_freq_month'] = group_design_matrix['THC_freq_month'] - group_design_matrix['THC_freq_month'].mean()\n",
    "            else:\n",
    "                dict_MM_MRIvisit_THC = non_img_data[non_img_data['SSS.CHR.Time_point'] == 'One year'].groupby('IDS.CHR.Subject')['TLF.CHR.THC.Frequency_in_month'].agg(\"first\").to_dict()\n",
    "                dict_MM_MRIvisit_THC_num = {k:freq_dict[v] for k,v in dict_MM_MRIvisit_THC.items()}\n",
    "                group_design_matrix['THC_freq_month'] = df_subs['subs'].map(dict_MM_MRIvisit_THC_num)\n",
    "                group_design_matrix['THC_freq_month'] = group_design_matrix['THC_freq_month'] - group_design_matrix['THC_freq_month'].mean()\n",
    "\n",
    "                \n",
    "    return group_design_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_threshold_map(z_map,mask):\n",
    "    threshold_map, threshold = threshold_stats_img(z_map, mask_img=mask, alpha=.05, height_control='fdr', cluster_threshold=0, two_sided=True)\n",
    "    return (threshold_map, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_axial_slices(threshold_map, threshold, MNI_template_used, contrast, group, ses, part_count):\n",
    "\n",
    "    #display = plotting.plot_stat_map(threshold_map, bg_img = MNI_template_used, display_mode='z', cut_coords=[-28,0,28,40,60],threshold=threshold,vmax=7)\n",
    " \n",
    "    if threshold:\n",
    "        display = plotting.plot_stat_map(threshold_map, bg_img = MNI_template_used, display_mode='z', cut_coords=[0],vmax=7,threshold=threshold, colorbar=True)\n",
    "        rounded_thresh = round(threshold,2)\n",
    "        display.title(text=f'Thresholded z map; \"{contrast}\" contrast of {group} at {ses}, exp. fdr = .05, z_thresh = {rounded_thresh}, n = {part_count}', size=14, y=1.2)\n",
    "    else:\n",
    "        display = plotting.plot_stat_map(threshold_map, bg_img = MNI_template_used, display_mode='z', cut_coords=[0],threshold=None, colorbar=True)\n",
    "        display.title(text=f'Effect size map; \"{contrast}\" contrast of {group} at {ses}, n = {part_count}', size=14, y=1.2)\n",
    "    \n",
    "    return display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_glass_brains(threshold_map, threshold, contrast, group, ses, part_count):\n",
    "    display = plot_glass_brain(\n",
    "    stat_map_img = threshold_map,\n",
    "    threshold=threshold,\n",
    "    colorbar=True,\n",
    "    display_mode='ortho',\n",
    "    plot_abs=False)\n",
    "    rounded_thresh = round(threshold,2)\n",
    "    display.title(text=f'Z scores on glass brain; \"{contrast}\" contrast of {group} at {ses}, exp. fdr = .05, z_thresh = {rounded_thresh}, n = {part_count}',\n",
    "                  size=14, y=1.2)\n",
    "    return display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_group_level_outputs(standardized_effect_map, stat_map, threshold, group, contrast, task, ses, participants, covariates):\n",
    "    print(f'Now saving output for group {group} for task {task} and contrast {contrast} in session {ses}')\n",
    "\n",
    "    \n",
    "    #check that output does not exist\n",
    "    output = glob.glob(f'../../../derivatives/task_analysis_volume/group_level/group-{group}/ses-{ses}/task-{task}/group-{group}_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_std_effect_size.nii.gz')\n",
    "    if output: #checks if list is not empty\n",
    "        print(f'At least partial group level output exists for group {group}, session {ses} and task {task}. This must be deleted for before generating new output.')\n",
    "        return \n",
    "    \n",
    "    \n",
    "    #create paths to output dir if not exist\n",
    "    derivatives_path = '../../../derivatives'\n",
    "    nilearn_output_path = os.path.join(derivatives_path, 'task_analysis_volume','group_level',f'group-{group}',f'ses-{ses}',f'task-{task}')\n",
    "    if not os.path.isdir(nilearn_output_path):\n",
    "        os.makedirs (nilearn_output_path)\n",
    "\n",
    "        \n",
    "    #save contrast maps to files\n",
    "    standardized_effect_map.to_filename(f'../../../derivatives/task_analysis_volume/group_level/group-{group}/ses-{ses}/task-{task}/group-{group}_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_std_effect_size.nii.gz')\n",
    "    stat_map.to_filename(f'../../../derivatives/task_analysis_volume/group_level/group-{group}/ses-{ses}/task-{task}/group-{group}_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_z_score.nii.gz')\n",
    "\n",
    "    with open(f'../../../derivatives/task_analysis_volume/group_level/group-{group}/ses-{ses}/task-{task}/group-{group}_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_FDR_threshold.txt', 'x') as f:\n",
    "        f.writelines(str(threshold) + '\\n')\n",
    "    \n",
    "    with open(f'../../../derivatives/task_analysis_volume/group_level/group-{group}/ses-{ses}/task-{task}/group-{group}_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_covariates.txt', 'x') as f:\n",
    "        f.writelines('\\n'.join(covariates) + '\\n')\n",
    "        \n",
    "    with open(f'../../../derivatives/task_analysis_volume/group_level/group-{group}/ses-{ses}/task-{task}/group-{group}_ses-{ses}_task-{task}_rec-unco_contrast-{contrast}_participants.txt', 'x') as f:\n",
    "        f.writelines('\\n'.join(participants) + '\\n')\n",
    "    \n",
    "    print(f'{group} {task} {ses} {contrast}: processing done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faad21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_effect_map(effect_map,effect_variance_map,N):\n",
    "    \n",
    "    #load effect map and variance as arrays\n",
    "    effect_map_nimg = nimg.load_img(effect_map)\n",
    "    effect_map_signal = effect_map_nimg.get_fdata(dtype='f4')\n",
    "    \n",
    "    effect_variance_map_nimg = nimg.load_img(effect_variance_map)\n",
    "    effect_variance_map_signal = effect_variance_map_nimg.get_fdata(dtype='f4')\n",
    "    effect_std_dev_map_signal = np.sqrt(effect_variance_map_signal)\n",
    "    \n",
    "    #calculate h to avoid biasing std. effect due to small sample\n",
    "    h = math.gamma((N-1)/2)/(np.sqrt((N-1)/2)*math.gamma((N-2)/2))\n",
    "    \n",
    "    #calculate std effect map\n",
    "    #avoid division by 0 due to mask\n",
    "    standardized_effect_map = (np.divide(effect_map_signal, effect_std_dev_map_signal, out=np.zeros_like(effect_map_signal), where=(effect_std_dev_map_signal != 0)))*h\n",
    "    \n",
    "    #get the affine matrix\n",
    "    affine_matrix = effect_map_nimg.affine\n",
    "    \n",
    "    #save as nifti again to be able to display below\n",
    "    standardized_effect_map_nifti = nibabel.Nifti1Image(standardized_effect_map, affine=affine_matrix)\n",
    "\n",
    "    return standardized_effect_map_nifti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c62808",
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mid'\n",
    "sessions = ['baseline','1year']\n",
    "groups = ['HC','MM']\n",
    "MNI_template_path = '../templates/tpl-MNI152NLin6Asym_res-01_desc-brain_T1w.nii.gz'\n",
    "MNI_template_used = nibabel.load(MNI_template_path)\n",
    "\n",
    "\n",
    "if task == 'mid':\n",
    "        contrasts = [ 'HiRewCue-NeuCue', #high reward anticipation -- paper A2, ABCD\n",
    "                      'LoRewCue-NeuCue', #low reward anticipation -- ABCD\n",
    "                      'RewCue-NeuCue', #combined reward anticipation -- paper A1\n",
    "                      'HiRewCue-LoRewCue', #high vs. low reward anticipation -- paper A3, ABCD\n",
    "                      'HiRewCue-Baseline', #high reward anticipation vs. baseline -- paper A4\n",
    "                      'HiLossCue-NeuCue', #high loss anticipation -- paper A5, ABCD\n",
    "                      'LoLossCue-NeuCue', #low loss anticipation -- ABCD\n",
    "                      'HiLossCue-LoLossCue', #high vs. low loss anticipation -- ABCD\n",
    "                      'HiWin-NeuHit', #high reward outcome cp. to neutral hit -- paper O6\n",
    "                      'Win-NoWin', #combined reward outcome cp. to combined reward miss -- ABCD\n",
    "                      'HiLoss-NeuHit', #high loss cp. to neutral hit -- paper O7\n",
    "                      'Loss-AvoidLoss', #combined loss cp. to combined avoid loss -- ABCD\n",
    "                     ] \n",
    "      \n",
    "elif task == 'sst':\n",
    "    contrasts=['SuccStop-Go','UnsuccStop-Go','UnsuccStop-SuccStop']\n",
    "    \n",
    "elif task == 'nback':\n",
    "    contrasts=['twoback-zeroback']\n",
    "\n",
    "#options are age, sex, total_cudit, THC_freq_month at the moment (HC is 0 for total_cudit and THC_freq)\n",
    "#covariates = ['sex','age','total_cudit', 'THC_freq_month']\n",
    "covariates = []\n",
    "\n",
    "#subgroup = 'dipstick_THC'\n",
    "subgroup = ''\n",
    "\n",
    "#set to true if MM participants with CUD at baseline should be excluded from the analysis\n",
    "rm_CUD = True\n",
    "\n",
    "#set this to the ones we want to include with values for fd_mean and fd_perc as keys\n",
    "desired_outliers = {'tsnr':[],'snr':[],'gsr_x':[],'gsr_y':[],'fd_mean':0.2,'fd_perc':0.3,'motion_outlier_cutoff':0.3}\n",
    "#desired_outliers = {}\n",
    "\n",
    "\n",
    "#loop through all sessions, groups, contrasts\n",
    "for ses in sessions:\n",
    "    for group in groups:        \n",
    "        for contrast in contrasts:\n",
    "            #get group average group level contrast as the output\n",
    "            contrast_outputs, mask, part_count, participants = get_group_average_contrast(contrast,ses,group,task,covariates,subgroup,rm_CUD,desired_outliers)\n",
    "            \n",
    "            #account for HC not having 1year scans\n",
    "            if contrast_outputs:\n",
    "                \n",
    "                print(f'For task contrast {contrast}, we will print the covariate-based contrasts in the following order: {contrast_outputs.keys()}:')\n",
    "                \n",
    "                for slope in contrast_outputs.keys():\n",
    "                    \n",
    "                    contrast_output = contrast_outputs[slope]\n",
    "                    \n",
    "                    #compute FDR-corrected z-score thresholdmap\n",
    "                    #note that this does not have a cluster threshold\n",
    "                    #this image is masked by the mask intersection across all subjects\n",
    "                    stat_map = contrast_output['z_score']\n",
    "                    effect_map = contrast_output['effect_size']\n",
    "                    effect_variance_map = contrast_output['effect_variance']\n",
    "                    \n",
    "                    standardized_effect_map = get_standardized_effect_map(effect_map,effect_variance_map,part_count)\n",
    "                    \n",
    "                    threshold_map, threshold = fdr_threshold_map(stat_map, mask)\n",
    "                    \n",
    "                    #save group average output for fsleyes plotting\n",
    "                    if slope == 'group_average' and subgroup == '':\n",
    "                        save_group_level_outputs(standardized_effect_map, stat_map, threshold, group, contrast, task, ses, participants, covariates)\n",
    "           \n",
    "                        #make desired plots\n",
    "                        #mask_axial_slices = plot_axial_slices(mask, None, MNI_template_used, contrast, group, ses, part_count)\n",
    "\n",
    "                        effect_axial_slices = plot_axial_slices(standardized_effect_map, None, MNI_template_used, contrast, group, ses, part_count)\n",
    "\n",
    "                        zmap_axial_slices = plot_axial_slices(threshold_map, threshold, MNI_template_used, contrast, group, ses, part_count)\n",
    "\n",
    "                        #zmap_glass_brain = plot_glass_brains(threshold_map, threshold, contrast, group, ses, part_count)\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99536dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc18c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940d848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group difference model for control vs. MM at baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a26c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_group_difference_contrast(contrast,ses,groups,task,covariates,subgroup, rm_CUD,desired_outliers):\n",
    "    \n",
    "    #get individual effect size maps and a design matrix per group\n",
    "    effect_size_maps=[] #list of both groups effect size maps\n",
    "    group_sizes=[] #list of the two group sizes\n",
    "    group_design_matrices=[] #get separate design matrices per group that will be combined\n",
    "    for group in groups:\n",
    "        excluded_subs_by_group_runs_dict = get_excluded_runs(desired_outliers,contrast,ses,group,task)\n",
    "        effect_size_maps_by_group, all_subs = get_effect_size_maps(contrast,ses,group,task,excluded_subs_by_group_runs_dict)\n",
    "        \n",
    "        #remove participants with CUD at baseline (only applies to MM)\n",
    "        if rm_CUD:\n",
    "            effect_size_maps_by_group, all_subs = rm_CUD_baseline(effect_size_maps_by_group, all_subs)\n",
    "        \n",
    "        #note that subject formatting is changed slightly to match that of the non_imaging_data csv file\n",
    "        #need these to make design matrix\n",
    "        subs_csv = ['_'.join([s for s in re.split(r'(MM|HC)', sub) if s]) for sub in all_subs]\n",
    "       \n",
    "        if subgroup:\n",
    "            effect_size_maps_by_group, subs_csv, all_subs = select_subgroup(subgroup, effect_size_maps_by_group, subs_csv, all_subs)\n",
    "\n",
    "        effect_size_maps += effect_size_maps_by_group\n",
    "        group_sizes.append(len(effect_size_maps_by_group))\n",
    "        \n",
    "        #create design matrix per group\n",
    "        #note that the columns that need to stay separate when stacking the design matrices later get renamed\n",
    "        indv_design_matrix = create_group_design_matrix(subs_csv,group,ses,covariates)\n",
    "        indv_design_matrix.rename({'group_average': f'group_average_{group}'}, axis='columns',inplace=True)\n",
    "        \n",
    "        group_design_matrices.append(indv_design_matrix)\n",
    "    \n",
    "    \n",
    "    #stack design matrices of the two groups to make one large design matrix\n",
    "    #note again that group_average or Male&Female columns, which were renamed, will not get stacked\n",
    "    #thus, these will partially contain NaNs, which will be replaced by 0s\n",
    "    group_design_matrix = pd.concat(group_design_matrices).replace(np. nan,0) \n",
    "\n",
    "    print(group_design_matrix)\n",
    "    \n",
    "    #grab masks needed (both groups at baseline)\n",
    "    masks = glob.glob(f'../../../derivatives/ses-{ses}/sub-*/ses-{ses}/func/sub-*task-{task}*rec-unco*run-*_*{space}*brain_mask.nii.gz')\n",
    "\n",
    "    #make intersection of the masks\n",
    "    #threshold=1 corresponds to keeping the intersection of all masks, whereas threshold=0 is the union of all masks\n",
    "    mask = intersect_masks(masks, threshold=1, connected=True)\n",
    "    \n",
    "    \n",
    "    #define group level model\n",
    "    second_level_model = SecondLevelModel(mask_img=mask, target_affine=None, target_shape=None, \n",
    "                                      smoothing_fwhm=None, memory_level=1, \n",
    "                                      verbose=0, n_jobs=-2, minimize_memory=False)\n",
    "    #fit model\n",
    "    second_level_model = second_level_model.fit(\n",
    "        effect_size_maps,\n",
    "        design_matrix=group_design_matrix,\n",
    "    )\n",
    "\n",
    "    #make list of possible contrasts: HC only, MM only, MM-HC\n",
    "    contrast_matrix = np.eye(group_design_matrix.shape[1])\n",
    "    grp_contrasts = dict([(column, contrast_matrix[i])\n",
    "                  for i, column in enumerate(group_design_matrix.columns)])\n",
    "     \n",
    "    grp_contrasts['group_average'] = grp_contrasts['group_average_MM']-grp_contrasts['group_average_HC']\n",
    "        \n",
    "    print(grp_contrasts)\n",
    "    \n",
    "    \n",
    "    contrast_outputs = {}\n",
    "    \n",
    "    for key in grp_contrasts.keys():\n",
    "        if 'group_average_' not in key:\n",
    "            contrast_output = second_level_model.compute_contrast(second_level_contrast=grp_contrasts[key], \n",
    "                                                              second_level_stat_type=None, output_type='all')\n",
    "            contrast_outputs[key] = contrast_output\n",
    "    \n",
    "    #for printing the covariates\n",
    "    if covariates == '':\n",
    "        covariates = 'nothing'\n",
    "        \n",
    "    print('Processing done of the baseline difference of MM and HC for: '+' '.join([contrast,task]))\n",
    "    print(f'Controlled for {covariates}')\n",
    "\n",
    "    \n",
    "    return (contrast_outputs, mask,(group_sizes[0],group_sizes[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=['MM','HC']\n",
    "ses='baseline'\n",
    "task='mid'\n",
    "space='MNI152NLin6Asym' #change if desired\n",
    "covariates_list = [[],['sex'],['age'],['THC_freq_month'],['total_cudit'],['age','sex'],['THC_freq_month','age','sex'],['total_cudit','age','sex'] ]\n",
    "#covariates_list = [[]]\n",
    "MNI_template_used = nibabel.load('../templates/tpl-MNI152NLin6Asym_res-02_desc-brain_T1w.nii.gz')\n",
    "\n",
    "\n",
    "if task == 'mid':\n",
    "        contrasts = [ 'HiRewCue-NeuCue', #high reward anticipation -- paper A2, ABCD\n",
    "                      'LoRewCue-NeuCue', #low reward anticipation -- ABCD\n",
    "                      'RewCue-NeuCue', #combined reward anticipation -- paper A1\n",
    "                      'HiRewCue-LoRewCue', #high vs. low reward anticipation -- paper A3, ABCD\n",
    "                      'HiRewCue-Baseline', #high reward anticipation vs. baseline -- paper A4\n",
    "                      'HiLossCue-NeuCue', #high loss anticipation -- paper A5, ABCD\n",
    "                      'LoLossCue-NeuCue', #low loss anticipation -- ABCD\n",
    "                      'HiLossCue-LoLossCue', #high vs. low loss anticipation -- ABCD\n",
    "                      'HiWin-NeuHit', #high reward outcome cp. to neutral hit -- paper O6\n",
    "                      'Win-NoWin', #combined reward outcome cp. to combined reward miss -- ABCD\n",
    "                      'HiLoss-NeuHit', #high loss cp. to neutral hit -- paper O7\n",
    "                      'Loss-AvoidLoss', #combined loss cp. to combined avoid loss -- ABCD\n",
    "                     ] \n",
    "    \n",
    "elif task == 'sst':\n",
    "    contrasts=['SuccStop-Go','UnsuccStop-Go','UnsuccStop-SuccStop']\n",
    "\n",
    "elif task == 'nback':\n",
    "    contrasts=['twoback-zeroback']\n",
    "\n",
    "#since we're comparing to HC, setting to only MCC subgroup doesn't make sense   \n",
    "subgroup = ''\n",
    "\n",
    "#set to true if MM participants with CUD at baseline should be excluded from the analysis\n",
    "rm_CUD = True\n",
    "\n",
    "#set this to the ones we want to include with values for fd_mean and fd_perc as keys\n",
    "desired_outliers = {'tsnr':[],'snr':[],'gsr_x':[],'gsr_y':[],'fd_mean':0.2,'fd_perc':0.3,'motion_outlier_cutoff':0.3}\n",
    "#desired_outliers = {}\n",
    "\n",
    "\n",
    "#TO: loop through all contrasts\n",
    "for covariates in covariates_list:\n",
    "    for contrast in contrasts:\n",
    "        #compute group difference contrast\n",
    "        contrast_outputs, mask, part_count = get_baseline_group_difference_contrast(contrast,ses,groups,task,covariates,subgroup,rm_CUD,desired_outliers)\n",
    "        print(covariates)\n",
    "        \n",
    "        print(f'We will print the covariate-based contrasts in the following order: {contrast_outputs.keys()}:')\n",
    "\n",
    "        for key in contrast_outputs.keys():\n",
    "            \n",
    "            contrast_output = contrast_outputs[key]\n",
    "            \n",
    "            #calculate fdr-thresholded map\n",
    "            threshold_map, threshold = fdr_threshold_map(contrast_output['z_score'], mask)\n",
    "            \n",
    "            if not np.isinf(threshold):\n",
    "                \n",
    "                #make visualizations\n",
    "                fig, ax = plt.subplots()\n",
    "                fig.text(0.5, 0.5, f\"{contrast}, controlled for: {covariates}, showing: {key}\", ha='center', va='center', fontsize=16)\n",
    "                ax.axis('off')\n",
    "                \n",
    "                #make visualizations\n",
    "                axial_slices = plot_axial_slices(threshold_map, threshold, MNI_template_used, contrast, 'MM minus HC', ses, part_count)\n",
    "                glass_brain = plot_glass_brains(threshold_map, threshold, contrast, 'MM minus HC', ses, part_count)\n",
    "\n",
    "                #effect_axial_slices = plot_axial_slices(contrast_output['effect_size'], None, MNI_template_used, contrast, group, ses, part_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275c4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6c96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2293d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAIRED group difference model for MM at 1year vs. baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9eb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MM_paired_group_difference_contrast(contrast,sessions,group,task,covariates,subgroup,rm_CUD,desired_outliers):\n",
    "        \n",
    "    #get individual effect size maps by subjects\n",
    "    effect_size_maps_by_ses = [] #list of two dictionaries, one per ses, both with subjects as keys and their effect size paths as values\n",
    "    \n",
    "    for ses in sessions:\n",
    "        excluded_subs_runs_dict = get_excluded_runs(desired_outliers,contrast,ses,group,task)\n",
    "        \n",
    "        effect_size_maps, all_subs = get_effect_size_maps(contrast,ses,group,task,excluded_subs_runs_dict)\n",
    "        #remove participants with CUD at baseline (only applies to MM)\n",
    "        if rm_CUD:\n",
    "            effect_size_maps, all_subs = rm_CUD_baseline(effect_size_maps, all_subs)\n",
    "        effect_size_by_subj_dict = dict(zip(all_subs, effect_size_maps))\n",
    "        effect_size_maps_by_ses.append(effect_size_by_subj_dict)\n",
    "\n",
    "    #find subjects with both baseline and 1year scans\n",
    "    subjs_with_both_ses = list(set(effect_size_maps_by_ses[0].keys()).intersection(set(effect_size_maps_by_ses[1].keys())))\n",
    "    \n",
    "    #subtract baseline from 1year scans\n",
    "    #save as list of nii effect size maps\n",
    "    effect_size_maps=[]\n",
    "    for sub in subjs_with_both_ses:\n",
    "        effect_size_map_0 = nibabel.load(effect_size_maps_by_ses[0][sub])\n",
    "        effect_size_map_1 = nibabel.load(effect_size_maps_by_ses[1][sub])\n",
    "        diff_img = math_img(\"img0 - img1\", img0=effect_size_map_0, img1=effect_size_map_1)\n",
    "        effect_size_maps.append(diff_img)\n",
    "\n",
    "    #rewrite subject names so they match the csv file\n",
    "    subs_csv = ['_'.join([s for s in re.split(r'(MM|HC)', sub) if s]) for sub in subjs_with_both_ses]\n",
    "    \n",
    "    if subgroup:\n",
    "        effect_size_maps, subs_csv, subjs_with_both_ses = select_subgroup(subgroup, effect_size_maps, subs_csv, subjs_with_both_ses)\n",
    "    \n",
    "    part_count = len(subjs_with_both_ses)\n",
    "    \n",
    "    #get design matrix for the group\n",
    "    #if sex present, will include Male/Female; otherwise will include a column of all 1s called group average\n",
    "    group_design_matrix = create_group_design_matrix(subs_csv,group,ses,covariates)\n",
    "\n",
    "    #grab masks needed (all MM, both 1year and baseline) for the included subjects\n",
    "    masks = []\n",
    "    for sub in subjs_with_both_ses:\n",
    "        masks+=glob.glob(f'../../../derivatives/ses-*/sub-{sub}*/ses-*/func/sub-{group}*task-{task}*rec-unco*run-1_*{space}*brain_mask.nii.gz')\n",
    "\n",
    "    #make intersection of the masks\n",
    "    #threshold=1 corresponds to keeping the intersection of all masks, whereas threshold=0 is the union of all masks\n",
    "    mask = intersect_masks(masks, threshold=1, connected=True)\n",
    "    \n",
    "    #define group level model\n",
    "    #can also take group smoothing out if preferred (helps remove noise for noisier tasks)\n",
    "    second_level_model = SecondLevelModel(mask_img=mask, target_affine=None, target_shape=None, \n",
    "                                      smoothing_fwhm=4.0, memory_level=1, \n",
    "                                      verbose=0, n_jobs=-2, minimize_memory=False)\n",
    "    \n",
    "    #fit model\n",
    "    second_level_model = second_level_model.fit(effect_size_maps,design_matrix=group_design_matrix,)\n",
    "        \n",
    "\n",
    "    #define contrast matrix to include single column of 1s \n",
    "    contrast_matrix = np.eye(group_design_matrix.shape[1])\n",
    "    grp_contrasts = dict([(column, contrast_matrix[i])\n",
    "                  for i, column in enumerate(group_design_matrix.columns)])\n",
    "           \n",
    "    print(grp_contrasts)\n",
    "    \n",
    "    \n",
    "    contrast_outputs = {}\n",
    "        \n",
    "    for key in grp_contrasts.keys():\n",
    "        #compute all the group contrasts\n",
    "        contrast_output = second_level_model.compute_contrast(second_level_contrast=grp_contrasts[key], \n",
    "                                                              second_level_stat_type=None, output_type='all')\n",
    "        contrast_outputs[key] = contrast_output\n",
    "        \n",
    "\n",
    "    #for printing the covariates\n",
    "    if covariates == '':\n",
    "        covariates = 'nothing'\n",
    "        \n",
    "    print('Processing done of MM 1year vs. baseline for: '+' '.join([contrast,task]))\n",
    "    print(f'Controlled for {covariates}')\n",
    "\n",
    "    return (contrast_outputs, mask, part_count)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions=['1year','baseline']\n",
    "task='sst'\n",
    "group='MM'\n",
    "covariates_list = [[],['sex'],['age'],['THC_freq_month'],['total_cudit'],['age','sex'],['THC_freq_month','age','sex'],['total_cudit','age','sex'] ]\n",
    "#covariates_list = [[]]\n",
    "space='MNI152NLin6Asym' #change if desired\n",
    "MNI_template_used = nibabel.load('../templates/tpl-MNI152NLin6Asym_res-02_desc-brain_T1w.nii.gz')\n",
    "\n",
    "\n",
    "if task == 'mid':\n",
    "        contrasts = [ 'HiRewCue-NeuCue', #high reward anticipation -- paper A2, ABCD\n",
    "                      'LoRewCue-NeuCue', #low reward anticipation -- ABCD\n",
    "                      'RewCue-NeuCue', #combined reward anticipation -- paper A1\n",
    "                      'HiRewCue-LoRewCue', #high vs. low reward anticipation -- paper A3, ABCD\n",
    "                      'HiRewCue-Baseline', #high reward anticipation vs. baseline -- paper A4\n",
    "                      'HiLossCue-NeuCue', #high loss anticipation -- paper A5, ABCD\n",
    "                      'LoLossCue-NeuCue', #low loss anticipation -- ABCD\n",
    "                      'HiLossCue-LoLossCue', #high vs. low loss anticipation -- ABCD\n",
    "                      'HiWin-NeuHit', #high reward outcome cp. to neutral hit -- paper O6\n",
    "                      'Win-NoWin', #combined reward outcome cp. to combined reward miss -- ABCD\n",
    "                      'HiLoss-NeuHit', #high loss cp. to neutral hit -- paper O7\n",
    "                      'Loss-AvoidLoss', #combined loss cp. to combined avoid loss -- ABCD\n",
    "                     ] \n",
    "    \n",
    "elif task == 'sst':\n",
    "    contrasts=['SuccStop-Go','UnsuccStop-Go','UnsuccStop-SuccStop']\n",
    "\n",
    "elif task == 'nback':\n",
    "    contrasts=['twoback-zeroback']\n",
    "    \n",
    "    \n",
    "subgroup = ''\n",
    "#subgroup = 'dipstick_THC'\n",
    "\n",
    "#set to true if MM participants with CUD at baseline should be excluded from the analysis\n",
    "rm_CUD = True\n",
    "\n",
    "#set this to the ones we want to include with values for fd_mean and fd_perc as keys\n",
    "desired_outliers = {'tsnr':[],'snr':[],'gsr_x':[],'gsr_y':[],'fd_mean':0.2,'fd_perc':0.3,'motion_outlier_cutoff':0.3}\n",
    "#desired_outliers = {}\n",
    "\n",
    "\n",
    "for covariates in covariates_list:\n",
    "    for contrast in contrasts:\n",
    "        contrast_outputs, mask, part_count = get_MM_paired_group_difference_contrast(contrast,sessions,group,task,covariates,subgroup,rm_CUD,desired_outliers)\n",
    "\n",
    "        print(f'We will print the covariate-based contrasts in the following order: {contrast_outputs.keys()}:')\n",
    "\n",
    "        for key in contrast_outputs.keys():\n",
    "            \n",
    "            contrast_output = contrast_outputs[key]\n",
    "  \n",
    "            #calculate fdr-thresholded map\n",
    "            threshold_map, threshold = fdr_threshold_map(contrast_output['z_score'], mask)\n",
    "            \n",
    "            if not np.isinf(threshold):\n",
    "                \n",
    "                #make visualizations\n",
    "                fig, ax = plt.subplots()\n",
    "                fig.text(0.5, 0.5, f\"{contrast}, controlled for: {covariates}, showing: {key}\", ha='center', va='center', fontsize=16)\n",
    "                ax.axis('off')\n",
    "\n",
    "                axial_slices = plot_axial_slices(threshold_map, threshold, MNI_template_used, contrast, group, '1year minus baseline', part_count)\n",
    "                glass_brain = plot_glass_brains(threshold_map, threshold, contrast, group, '1year minus baseline', part_count)\n",
    "                #effect_axial_slices = plot_axial_slices(contrast_output['effect_size'], None, MNI_template_used, contrast, group, '1year minus baseline', part_count)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
